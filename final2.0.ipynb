{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports that are needed \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import gym_chess\n",
    "import random\n",
    "import numpy as np\n",
    "import chess\n",
    "import chess.svg\n",
    "from IPython.display import display, SVG\n",
    "import os\n",
    "from datetime import datetime\n",
    "import math\n",
    "import chess.pgn\n",
    "\n",
    "# Reset PyTorch's CUDA state\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class ActorCNN(nn.Module):\n",
    "    def __init__(self, output_size=4672):\n",
    "        super(ActorCNN, self).__init__()\n",
    "        # Input channels = 12 (6 piece types * 2 colors)\n",
    "        self.conv1 = nn.Conv2d(in_channels=12, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # After convolution, the board is still 8x8 in spatial dimension\n",
    "        # Flatten for the fully connected layers:\n",
    "        self.fc1 = nn.Linear(in_features=64 * 8 * 8, out_features=256)\n",
    "        self.fc2 = nn.Linear(256, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, 12, 8, 8)\n",
    "        \"\"\"\n",
    "        # Convolution layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # shape becomes (batch_size, 64*8*8)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Output layer -> softmax for a probability distribution\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "class CriticCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CriticCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(12, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, 12, 8, 8)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x  # No activation; can be negative or positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate_network(network, mutation_rate):\n",
    "    if isinstance(network, ActorCNN):\n",
    "        new_network = ActorCNN(output_size=4672)  # Using fixed sizes for chess\n",
    "    else:\n",
    "        new_network = CriticCNN()  # Using fixed size for chess\n",
    "        \n",
    "    new_network.load_state_dict(network.state_dict())\n",
    "    \n",
    "    for param in new_network.parameters():\n",
    "        if torch.rand(1) < mutation_rate:\n",
    "            param.data += torch.randn_like(param) * 0.1\n",
    "            \n",
    "    return new_network\n",
    "# Initialize model and test\n",
    "output_size = 4672\n",
    "actor_net = ActorCNN(output_size=output_size)\n",
    "\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, board, parent=None, prior=0, device=None):\n",
    "        self.board = board\n",
    "        self.parent = parent\n",
    "        self.prior = prior\n",
    "        self.children = {}\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.state = board_to_3d_tensor(board).to(self.device)\n",
    "        self.move_number = board.fullmove_number  # Track move number for game phase\n",
    "\n",
    "    def expand(self, actor_net):\n",
    "        action_probs = actor_net(self.state.unsqueeze(0).to(self.device))\n",
    "        move_lookup = create_move_lookup()\n",
    "        for move in self.board.legal_moves:\n",
    "            next_board = self.board.copy()\n",
    "            next_board.push(move)\n",
    "            self.children[move] = MCTSNode(\n",
    "                next_board, \n",
    "                parent=self, \n",
    "                prior=action_probs[0][move_to_index(move, move_lookup)],\n",
    "                device=self.device\n",
    "            )\n",
    "\n",
    "    def select_child(self):\n",
    "        # Dynamic exploration constant based on game phase\n",
    "        if self.move_number < 10:\n",
    "            c_puct = 2.0  # Higher exploration in opening\n",
    "        elif self.move_number < 20:\n",
    "            c_puct = 1.5  # Moderate exploration in middlegame\n",
    "        else:\n",
    "            c_puct = 1.0  # Lower exploration in endgame\n",
    "\n",
    "        best_score = float('-inf')\n",
    "        best_child = None\n",
    "        \n",
    "        for move, child in self.children.items():\n",
    "            # UCB1 formula with prior probability\n",
    "            ucb_score = child.get_value() + c_puct * child.prior * \\\n",
    "                       (math.sqrt(self.visit_count) / (1 + child.visit_count))\n",
    "            \n",
    "            # Check if move is legal before evaluating captures and checks\n",
    "            if move in self.board.legal_moves:\n",
    "                # Create a copy of the board to safely check move properties\n",
    "                temp_board = self.board.copy()\n",
    "                temp_board.push(move)\n",
    "                \n",
    "                # Check for captures\n",
    "                if temp_board.is_capture(move):\n",
    "                    ucb_score += 0.5\n",
    "                \n",
    "                # Check for checks\n",
    "                if temp_board.is_check():\n",
    "                    ucb_score += 0.3\n",
    "                \n",
    "                if ucb_score > best_score:\n",
    "                    best_score = ucb_score\n",
    "                    best_child = (move, child)\n",
    "                    \n",
    "        return best_child\n",
    "\n",
    "    def get_value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "def mcts_search(board, actor_net, critic_net, num_simulations=150):\n",
    "    root = MCTSNode(board)\n",
    "    \n",
    "    for _ in range(num_simulations):\n",
    "        node = root\n",
    "        search_path = [node]\n",
    "        \n",
    "        while node.children and not node.board.is_game_over():\n",
    "            move, node = node.select_child()\n",
    "            search_path.append(node)\n",
    "            \n",
    "        if not node.board.is_game_over():\n",
    "            node.expand(actor_net)\n",
    "            \n",
    "        value = critic_net(node.state.unsqueeze(0))\n",
    "        \n",
    "        for node in search_path:\n",
    "            node.value_sum += value\n",
    "            node.visit_count += 1\n",
    "            \n",
    "    return max(root.children.items(), key=lambda x: x[1].visit_count)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, device=None):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Initialize empty buffers with numpy arrays\n",
    "        self.buffer = []\n",
    "        self.priorities = np.ones(capacity, dtype=np.float32)  # Initialize all priorities to 1\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Convert inputs to tensors and move to device\n",
    "        state = state.to(self.device)\n",
    "        if isinstance(action, torch.Tensor):\n",
    "            action = action.to(self.device)\n",
    "        else:\n",
    "            action = torch.tensor([action], device=self.device)\n",
    "        reward = torch.tensor([reward], device=self.device, dtype=torch.float32)\n",
    "        next_state = next_state.to(self.device)\n",
    "        done = torch.tensor([done], device=self.device, dtype=torch.float32)\n",
    "        \n",
    "        # Create experience tuple\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        \n",
    "        # Add experience to buffer\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.position] = experience\n",
    "        \n",
    "        # Update priority\n",
    "        self.priorities[self.position] = self.priorities[:len(self.buffer)].max()\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "\n",
    "# Add this method to your PrioritizedReplayBuffer class\n",
    "def __len__(self):\n",
    "    return len(self.buffer)\n",
    "\n",
    "def sample(self, batch_size):\n",
    "    # If buffer is not populated enough, return empty samples\n",
    "    if len(self.buffer) < batch_size:\n",
    "        return [], [], []\n",
    "    \n",
    "    # Calculate sampling probabilities based on priorities\n",
    "    priorities = self.priorities[:len(self.buffer)]\n",
    "    probabilities = priorities ** self.alpha\n",
    "    probabilities = probabilities / np.sum(probabilities)\n",
    "    \n",
    "    # Sample indices based on priorities\n",
    "    indices = np.random.choice(len(self.buffer), batch_size, p=probabilities, replace=False)\n",
    "    \n",
    "    # Calculate importance sampling weights\n",
    "    weights = (len(self.buffer) * probabilities[indices]) ** (-0.4)  # Beta=0.4\n",
    "    weights = weights / np.max(weights)  # Normalize weights\n",
    "    weights = torch.tensor(weights, dtype=torch.float32, device=self.device)\n",
    "    \n",
    "    # Get experiences\n",
    "    batch = [self.buffer[idx] for idx in indices]\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    # Stack tensors\n",
    "    states = torch.cat([s.unsqueeze(0) for s in states])\n",
    "    actions = torch.cat([a for a in actions])\n",
    "    rewards = torch.cat([r for r in rewards])\n",
    "    next_states = torch.cat([ns.unsqueeze(0) for ns in next_states])\n",
    "    dones = torch.cat([d for d in dones])\n",
    "    \n",
    "    experiences = (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    return experiences, indices, weights\n",
    "\n",
    "def update_priorities(self, indices, td_errors):\n",
    "    for idx, error in zip(indices, td_errors):\n",
    "        self.priorities[idx] = np.abs(error) + 1e-6  # Add small constant to avoid zero priority\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this in a new cell\n",
    "def adjust_training_parameters(episode, num_episodes):\n",
    "    # Gradually increase MCTS simulations\n",
    "    simulations = min(150 + (episode // 100) * 50, 500)\n",
    "    \n",
    "    # Adjust learning rate\n",
    "    lr = max(0.001 * (0.95 ** (episode // 100)), 0.0001)\n",
    "    \n",
    "    # Adjust exploration rate\n",
    "    exploration = max(0.1 * (0.95 ** (episode // 100)), 0.01)\n",
    "    \n",
    "    return simulations, lr, exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return torch.tensor(returns)\n",
    "\n",
    "class ModelPopulation:\n",
    "    def __init__(self, population_size=2):\n",
    "        self.population = []\n",
    "        for i in range(population_size):\n",
    "            actor = ActorCNN(output_size=4672)\n",
    "            critic = CriticCNN()\n",
    "            self.population.append((actor, critic))\n",
    "\n",
    "\n",
    "\n",
    "def run_tournament(population, games_per_match=10):\n",
    "    \"\"\"\n",
    "    population[i] = (actor_net_i, critic_net_i).\n",
    "    Scores[i] is the total round-robin score for model i.\n",
    "    \"\"\"\n",
    "    scores = {i: 0.0 for i in range(len(population))}\n",
    "    \n",
    "    # Round-robin tournament\n",
    "    for i in range(len(population)):\n",
    "        for j in range(i + 1, len(population)):\n",
    "            model1, model2 = population[i], population[j]\n",
    "            score_for_model1 = play_match(model1, model2, games_per_match)\n",
    "            # model1 is White, model2 is Black\n",
    "            scores[i] += score_for_model1\n",
    "            scores[j] += (games_per_match - score_for_model1)\n",
    "    \n",
    "    # Sort models by tournament performance\n",
    "    ranked_models = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ranked_models\n",
    "\n",
    "\n",
    "def play_match(model1, model2, num_games):\n",
    "    \"\"\"\n",
    "    Returns the total score earned by model1 (the White side) over `num_games`.\n",
    "    Each game:\n",
    "      - 1.0 for White win,\n",
    "      - 0.0 for Black win,\n",
    "      - 0.5 for draw.\n",
    "    \"\"\"\n",
    "    model1_score = 0.0\n",
    "    print(f\"Playing match between Model 1 (ActorCNN-CriticCNN) and Model 2 (ActorCNN-CriticCNN)\")\n",
    "\n",
    "    for game in range(num_games):\n",
    "        chess_board = chess.Board()\n",
    "        while not chess_board.is_game_over():\n",
    "            current_model = model1 if chess_board.turn else model2\n",
    "            move = mcts_search(chess_board, current_model[0], current_model[1])\n",
    "            chess_board.push(move)\n",
    "\n",
    "        # Final result string: \"1-0\", \"0-1\", or \"1/2-1/2\"\n",
    "        result = chess_board.result()\n",
    "        if result == \"1-0\":\n",
    "            # White (model1) wins\n",
    "            model1_score += 1.0\n",
    "        elif result == \"0-1\":\n",
    "            # Black (model2) wins\n",
    "            model1_score += 0.0\n",
    "        else:\n",
    "            # Draw\n",
    "            model1_score += 0.5\n",
    "\n",
    "    return model1_score\n",
    "\n",
    "\n",
    "\n",
    "def evolve_population(population, ranked_models, mutation_rate=0.01):\n",
    "    # Keep top 50% performers\n",
    "    survivors = [population[idx] for idx, _ in ranked_models[:len(population)//2]]\n",
    "    \n",
    "    # Create offspring with mutations\n",
    "    offspring = []\n",
    "    for model in survivors:\n",
    "        new_actor = mutate_network(model[0], mutation_rate)\n",
    "        new_critic = mutate_network(model[1], mutation_rate)\n",
    "        offspring.append((new_actor, new_critic))\n",
    "    \n",
    "    return survivors + offspring\n",
    "\n",
    "\n",
    "#Move Handling Functions\n",
    "def create_move_lookup():\n",
    "    moves = []\n",
    "    for from_square in range(64):\n",
    "        for to_square in range(64):\n",
    "            moves.append((from_square, to_square))\n",
    "    return moves\n",
    "\n",
    "def get_piece_value(piece):\n",
    "    piece_values = {\n",
    "        chess.PAWN: 10,\n",
    "        chess.KNIGHT: 30,\n",
    "        chess.BISHOP: 30,\n",
    "        chess.ROOK: 50,\n",
    "        chess.QUEEN: 90,\n",
    "        chess.KING: 0\n",
    "    }\n",
    "    return piece_values.get(piece.piece_type, 0)\n",
    "\n",
    "def select_legal_action(action_probs, legal_moves, board):\n",
    "    probs = action_probs.detach().cpu().numpy()[0]\n",
    "    legal_moves_list = list(legal_moves)\n",
    "    move_lookup = create_move_lookup()\n",
    "    \n",
    "    move_indices = []\n",
    "    move_weights = []\n",
    "    \n",
    "    for move in legal_moves_list:\n",
    "        from_square = move.from_square\n",
    "        to_square = move.to_square\n",
    "        \n",
    "        # Calculate move value based on captured piece\n",
    "        captured_piece = board.piece_at(to_square)\n",
    "        move_value = get_piece_value(captured_piece) if captured_piece else 0\n",
    "        \n",
    "        try:\n",
    "            idx = move_lookup.index((from_square, to_square))\n",
    "            move_indices.append(idx)\n",
    "            move_weights.append(move_value + 1)  # Add 1 to ensure non-zero probability\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    if not move_indices:\n",
    "        return random.choice(legal_moves_list)\n",
    "    \n",
    "    legal_probs = probs[move_indices]\n",
    "    # Multiply probabilities by piece values\n",
    "    weighted_probs = legal_probs * np.array(move_weights)\n",
    "    weighted_probs = np.clip(weighted_probs, 1e-10, 1.0)\n",
    "    \n",
    "    if weighted_probs.sum() == 0 or np.isnan(weighted_probs.sum()):\n",
    "        weighted_probs = np.ones_like(weighted_probs) / len(weighted_probs)\n",
    "    else:\n",
    "        weighted_probs = weighted_probs / weighted_probs.sum()\n",
    "    \n",
    "    selected_idx = np.random.choice(len(move_indices), p=weighted_probs)\n",
    "    return legal_moves_list[selected_idx]\n",
    "\n",
    "def move_to_index(move, move_lookup):\n",
    "    from_square = move.from_square\n",
    "    to_square = move.to_square\n",
    "    return move_lookup.index((from_square, to_square))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state = state.to(self.device)\n",
    "        action = torch.tensor([action], device=self.device)\n",
    "        reward = torch.tensor([reward], device=self.device)\n",
    "        next_state = next_state.to(self.device)\n",
    "        done = torch.tensor([done], device=self.device)\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            torch.cat(states),\n",
    "            torch.cat(actions),\n",
    "            torch.cat(rewards),\n",
    "            torch.cat(next_states),\n",
    "            torch.cat(dones)\n",
    "        )\n",
    "\n",
    "def board_to_3d_tensor(board):\n",
    "    \"\"\"\n",
    "    Converts a chess.Board() into a shape (12, 8, 8) tensor.\n",
    "    12 channels: [p, n, b, r, q, k, P, N, B, R, Q, K].\n",
    "    Each channel is an 8x8 plane with 1.0 where that piece is present.\n",
    "    \"\"\"\n",
    "    # Channels for black pieces: p, n, b, r, q, k\n",
    "    # Channels for white pieces: P, N, B, R, Q, K\n",
    "    piece_symbols = ['p','n','b','r','q','k','P','N','B','R','Q','K']\n",
    "    state = np.zeros((12, 8, 8), dtype=np.float32)\n",
    "\n",
    "    for square in range(64):\n",
    "        piece = board.piece_at(square)\n",
    "        if piece is not None:\n",
    "            # Find which channel this piece corresponds to\n",
    "            symbol = piece.symbol()  # e.g. 'P', 'n', etc.\n",
    "            channel_idx = piece_symbols.index(symbol)\n",
    "            row = square // 8\n",
    "            col = square % 8\n",
    "            state[channel_idx, row, col] = 1.0\n",
    "\n",
    "    return torch.tensor(state, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_enhanced_reward(board, move, is_checkmate=False):\n",
    "    base_reward = 0.0\n",
    "    \n",
    "    # High reward for checkmate\n",
    "    if is_checkmate:\n",
    "        return 100.0\n",
    "    \n",
    "    # Penalize draw conditions\n",
    "    if board.is_stalemate() or board.is_repetition(3) or board.halfmove_clock >= 50:\n",
    "        return -100.0\n",
    "    \n",
    "    # Material balance\n",
    "    material_diff = evaluate_material_difference(board)\n",
    "    base_reward += material_diff * 0.1\n",
    "    \n",
    "    # Piece activity\n",
    "    piece_activity = len(list(board.attacks(move.to_square)))\n",
    "    base_reward += piece_activity * 0.2\n",
    "    \n",
    "    # Center control\n",
    "    center_squares = {chess.E4, chess.E5, chess.D4, chess.D5}\n",
    "    extended_center = {chess.C3, chess.C4, chess.C5, chess.C6, chess.F3, chess.F4, chess.F5, chess.F6}\n",
    "    if move.to_square in center_squares:\n",
    "        base_reward += 1.0\n",
    "    elif move.to_square in extended_center:\n",
    "        base_reward += 0.5\n",
    "        \n",
    "            \n",
    "    # Capture rewards\n",
    "    captured_piece = board.piece_at(move.to_square)\n",
    "    if captured_piece:\n",
    "        capture_value = get_piece_value(captured_piece)\n",
    "        base_reward += capture_value\n",
    "        \n",
    "        # Bonus for capturing with less valuable pieces\n",
    "        from_piece = board.piece_at(move.from_square)\n",
    "        if from_piece and get_piece_value(from_piece) < capture_value:\n",
    "            base_reward += capture_value * 0.5\n",
    "            \n",
    "    # King safety (castling)\n",
    "    from_piece = board.piece_at(move.from_square)\n",
    "    if from_piece and from_piece.piece_type == chess.KING:\n",
    "        if chess.square_distance(move.from_square, move.to_square) > 1:\n",
    "            base_reward += 2.0\n",
    "            \n",
    "    return base_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_champion_model(save_path):\n",
    "    actor = ActorCNN(output_size=4672).to(device)\n",
    "    critic = CriticCNN().to(device)\n",
    "    \n",
    "    if os.path.exists(f\"{save_path}_actor.pth\"):\n",
    "        actor.load_state_dict(torch.load(f\"{save_path}_actor.pth\"))\n",
    "    if os.path.exists(f\"{save_path}_critic.pth\"):\n",
    "        critic.load_state_dict(torch.load(f\"{save_path}_critic.pth\"))\n",
    "        \n",
    "    return actor, critic\n",
    "\n",
    "\n",
    "def initialize_from_champion(champion_model, new_model):\n",
    "    new_model.load_state_dict(champion_model.state_dict())\n",
    "    return new_model\n",
    "\n",
    "def train_chess_ai(\n",
    "    num_episodes=100,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    actor_net=None,\n",
    "    critic_net=None,\n",
    "    save_path='./models/chess_ai'\n",
    "):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    champion_path = os.path.join(save_path, 'champion')\n",
    "    os.makedirs(champion_path, exist_ok=True)\n",
    "\n",
    "    memory = PrioritizedReplayBuffer(capacity=50000, device=device)\n",
    "        \n",
    "    # Load champion model if exists\n",
    "    if os.path.exists(f\"{save_path}_actor.pth\") and actor_net and critic_net:\n",
    "        champion_actor, champion_critic = load_champion_model(save_path)\n",
    "        actor_net = initialize_from_champion(champion_actor, actor_net)\n",
    "        critic_net = initialize_from_champion(champion_critic, critic_net)\n",
    "    \n",
    "    actor_net = actor_net or ActorCNN(output_size=4672)\n",
    "    critic_net = critic_net or CriticCNN()\n",
    "    actor_net = actor_net.to(device)\n",
    "    critic_net = critic_net.to(device)\n",
    "    \n",
    "    actor_optimizer = torch.optim.Adam(actor_net.parameters(), lr=0.001)\n",
    "    critic_optimizer = torch.optim.Adam(critic_net.parameters(), lr=0.001)\n",
    "    \n",
    "    memory = PrioritizedReplayBuffer(capacity=100000)\n",
    "    move_lookup = create_move_lookup()\n",
    "    \n",
    "    # Optional: opening/endgame knowledge\n",
    "    opening_book = {}\n",
    "    endgame_database = {}\n",
    "    \n",
    "    total_moves = 0\n",
    "    checkmates = 0\n",
    "    white_wins = 0\n",
    "    black_wins = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"\\nEpisode {episode + 1}/{num_episodes}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        board = chess.Board()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Dynamic training parameter adjustment\n",
    "        simulations, lr, exploration = adjust_training_parameters(episode, num_episodes)\n",
    "        for param_group in actor_optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        for param_group in critic_optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        while not board.is_game_over():\n",
    "            total_moves += 1\n",
    "            if total_moves % 10 == 0:\n",
    "                print(f\"Total moves: {total_moves}\")\n",
    "                print(board)\n",
    "            \n",
    "            state = board_to_3d_tensor(board).unsqueeze(0)\n",
    "            move = mcts_search(board, actor_net, critic_net, num_simulations=simulations)\n",
    "            board.push(move)\n",
    "            next_state = board_to_3d_tensor(board).unsqueeze(0)\n",
    "            \n",
    "            reward = calculate_enhanced_reward(board, move, board.is_checkmate())\n",
    "            total_reward += reward\n",
    "            \n",
    "            is_done = board.is_game_over()\n",
    "            memory.push(state, move_to_index(move, move_lookup), reward, next_state, is_done)\n",
    "            \n",
    "            # Checkmate handling\n",
    "            if board.is_checkmate():\n",
    "                checkmates += 1\n",
    "                if board.turn == chess.BLACK:\n",
    "                    white_wins += 1\n",
    "                else:\n",
    "                    black_wins += 1\n",
    "                print(\"\\nCheckmate!\")\n",
    "                print(f\"Winner: {'White' if board.turn == chess.BLACK else 'Black'}\")\n",
    "        \n",
    "        # Train networks\n",
    "        if hasattr(memory, 'buffer') and len(memory.buffer) > batch_size:\n",
    "            batch, indices, weights = memory.sample(batch_size)\n",
    "            # Only pass the batch to update_networks\n",
    "            update_networks(batch, actor_net, critic_net, actor_optimizer, critic_optimizer)\n",
    "\n",
    "        # Log progress\n",
    "        print(f\"\\nEpisode Summary:\")\n",
    "        print(f\"Total Reward: {total_reward:.2f}\")\n",
    "        print(f\"Checkmates: {checkmates}, White Wins: {white_wins}, Black Wins: {black_wins}\")\n",
    "        print(f\"Average Moves/Game: {total_moves / (episode + 1):.1f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            torch.save(actor_net.state_dict(), f'{save_path}_actor.pth')\n",
    "            torch.save(critic_net.state_dict(), f'{save_path}_critic.pth')\n",
    "    \n",
    "    return actor_net, critic_net, opening_book, endgame_database\n",
    "\n",
    "\n",
    "def evaluate_material_difference(board):\n",
    "    white_total = 0\n",
    "    black_total = 0\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            # Use the unified evaluation function\n",
    "            value = get_piece_value(piece)\n",
    "            if piece.color == chess.WHITE:\n",
    "                white_total += value\n",
    "            else:\n",
    "                black_total += value\n",
    "    return white_total - black_total\n",
    "\n",
    "# Reward function for chess\n",
    "def calculate_reward(board, move, is_checkmate=False):\n",
    "    base_reward = 0.0\n",
    "\n",
    "    # High reward for winning by checkmate\n",
    "    if is_checkmate:\n",
    "        return 100000.0  \n",
    "    \n",
    "    # Reward for moves that lead toward checkmate\n",
    "    if board.is_check():\n",
    "        reward += 500.0\n",
    "\n",
    "    # Penalize draw conditions\n",
    "    if board.is_stalemate() or board.is_repetition(3) or board.halfmove_clock >= 50:\n",
    "        return -100000.0  \n",
    "\n",
    "    # Reward for threatening opponent's king\n",
    "    if board.is_attacked_by(not board.turn, board.king(board.turn)):\n",
    "        reward += 2.0\n",
    "\n",
    "    # # Center control reward\n",
    "    # center_squares = {chess.E4, chess.E5, chess.D4, chess.D5}\n",
    "    # if move.to_square in center_squares:\n",
    "    #     base_reward += 0.5\n",
    "\n",
    "    # # Reward development of knights and bishops from their starting positions\n",
    "    # from_piece = board.piece_at(move.from_square)\n",
    "    # if from_piece and from_piece.piece_type in [chess.KNIGHT, chess.BISHOP]:\n",
    "    #     if move.from_square in [chess.B1, chess.G1, chess.B8, chess.G8]:\n",
    "    #         base_reward += 0.3\n",
    "\n",
    "    # # Reward for castling or moving the king significantly (for king safety)\n",
    "    # if from_piece and from_piece.piece_type == chess.KING:\n",
    "    #     if chess.square_distance(move.from_square, move.to_square) > 1:\n",
    "    #         base_reward += 1.0\n",
    "\n",
    "    # Reward for capturing an opponent piece\n",
    "    captured_piece = board.piece_at(move.to_square)\n",
    "    if captured_piece:\n",
    "        base_reward += get_piece_value(captured_piece)\n",
    "\n",
    "    # Use the material difference as a proxy for hanging pieces or overall advantage.\n",
    "    base_reward += evaluate_material_difference(board)\n",
    "\n",
    "    return base_reward\n",
    "\n",
    "# Set global device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def tournament(models, num_episodes=100, save_path='./tournament_results'):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    # Move all models to GPU\n",
    "    models = [model.to(device) for model in models]\n",
    "    print(f\"Running tournament on: {device}\")\n",
    "    \n",
    "    # Tournament statistics\n",
    "    scores = [0] * len(models)\n",
    "    wins = [0] * len(models)\n",
    "    losses = [0] * len(models)\n",
    "    draws = [0] * len(models)\n",
    "    games_played = 0\n",
    "    tournament_history = []\n",
    "    \n",
    "    env = ChessEnv()  # Make sure your env handles draw logic\n",
    "\n",
    "    # Round robin tournament\n",
    "    for round_num in range(2):  # Two rounds so each model plays both colors\n",
    "        for i in range(len(models)):\n",
    "            for j in range(len(models)):\n",
    "                if i != j:\n",
    "                    # print(f\"\\nRound {round_num + 1}: Model {i+1} vs Model {j+1}\")\n",
    "                    \n",
    "                    # Track match statistics for i vs j\n",
    "                    match_data = []\n",
    "                    total_score = 0\n",
    "                    \n",
    "                    for episode in range(num_episodes):\n",
    "                        # print(f\"Episode {episode + 1}/{num_episodes}\")\n",
    "                        \n",
    "                        # Reset environment for each episode\n",
    "                        env.reset()\n",
    "                        state = env.get_state().to(device)\n",
    "                        done = False\n",
    "                        moves = 0\n",
    "                        # Positive => advantage model i, Negative => advantage model j\n",
    "                        episode_score = 0\n",
    "\n",
    "                        while not done:\n",
    "                            moves += 1\n",
    "                            # Model i's turn\n",
    "                            action1 = models[i](state)\n",
    "                            reward_i, done = env.step(action1)\n",
    "                            episode_score += reward_i\n",
    "\n",
    "                            if done:\n",
    "                                # If the game ended after i's turn, break out\n",
    "                                break\n",
    "\n",
    "                            # Model j's turn\n",
    "                            state = env.get_state().to(device)\n",
    "                            action2 = models[j](state)\n",
    "                            reward_j, done = env.step(action2)\n",
    "                            # Subtract j's reward (score for j reduces i's net score)\n",
    "                            episode_score -= reward_j\n",
    "\n",
    "                            if not done:\n",
    "                                state = env.get_state().to(device)\n",
    "\n",
    "                        # One game is completed\n",
    "                        games_played += 1\n",
    "\n",
    "                        # Debug: Check final episode score\n",
    "                        # print(f\"DEBUG: Final episode_score = {episode_score}\")\n",
    "\n",
    "                        # Decide the winner based on final episode_score\n",
    "                        if episode_score > 0:\n",
    "                            wins[i] += 1\n",
    "                            losses[j] += 1\n",
    "                        elif episode_score < 0:\n",
    "                            wins[j] += 1\n",
    "                            losses[i] += 1\n",
    "                        else:\n",
    "                            draws[i] += 1\n",
    "                            draws[j] += 1\n",
    "\n",
    "                        total_score += episode_score\n",
    "                        \n",
    "                        match_data.append({\n",
    "                            'episode': episode + 1,\n",
    "                            'moves': moves,\n",
    "                            'score': episode_score\n",
    "                        })\n",
    "                        \n",
    "                        # Print statistics every 2 episodes (adjust as you like)\n",
    "                        if episode % 2 == 0:\n",
    "                            for k in range(len(models)):\n",
    "                                win_rate = (wins[k] / games_played) * 100 if games_played > 0 else 0\n",
    "                                loss_rate = (losses[k] / games_played) * 100 if games_played > 0 else 0\n",
    "                                draw_rate = (draws[k] / games_played) * 100 if games_played > 0 else 0\n",
    "                                # print(f\"\"\"Model {k+1}:\n",
    "                                #     Wins: {wins[k]} ({win_rate:.2f}%)\n",
    "                                #     Losses: {losses[k]} ({loss_rate:.2f}%)\n",
    "                                #     Draws: {draws[k]} ({draw_rate:.2f}%)\n",
    "                                #     Total Games: {games_played}\"\"\")\n",
    "\n",
    "                    # Tally the net score for i and j\n",
    "                    scores[i] += total_score\n",
    "                    scores[j] -= total_score\n",
    "\n",
    "                    tournament_history.append({\n",
    "                        'round': round_num + 1,\n",
    "                        'model1': i,\n",
    "                        'model2': j,\n",
    "                        'matches': match_data,\n",
    "                        'total_score': total_score\n",
    "                    })\n",
    "                    \n",
    "                    # Save results periodically\n",
    "                    np.save(f'{save_path}_history.npy', tournament_history)\n",
    "    \n",
    "    # Determine champion\n",
    "    champion_idx = scores.index(max(scores))\n",
    "    print(f\"\\nTournament Winner: Model {champion_idx + 1}\")\n",
    "    print(f\"Final Scores: {scores}\")\n",
    "    print(f\"Total Wins: {wins}\")\n",
    "    print(f\"Total Draws: {draws}\")\n",
    "    \n",
    "    return models[champion_idx], tournament_history\n",
    "\n",
    "def update_networks(batch, actor_net, critic_net, actor_optimizer, critic_optimizer):\n",
    "    # Unpack the batch tuple\n",
    "    experiences, indices, weights = batch\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    \n",
    "    # Update critic\n",
    "    with torch.no_grad():\n",
    "        next_values = critic_net(next_states)\n",
    "        target_values = rewards.unsqueeze(1) + (1 - dones.unsqueeze(1)) * 0.99 * next_values\n",
    "    \n",
    "    current_values = critic_net(states)\n",
    "    critic_loss = (weights.unsqueeze(1) * F.mse_loss(current_values, target_values, reduction='none')).mean()\n",
    "    \n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optimizer.step()\n",
    "    \n",
    "    # Update actor\n",
    "    action_probs = actor_net(states)\n",
    "    action_log_probs = torch.log(action_probs + 1e-10)\n",
    "    selected_action_log_probs = action_log_probs.gather(1, actions.unsqueeze(1))\n",
    "    \n",
    "    advantages = (target_values - current_values).detach()\n",
    "    actor_loss = -(weights.unsqueeze(1) * selected_action_log_probs * advantages).mean()\n",
    "    \n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizer.step()\n",
    "    \n",
    "    # Update priorities\n",
    "    td_errors = torch.abs(target_values - current_values).detach().cpu().numpy().flatten()\n",
    "    batch[1].update_priorities(indices, td_errors)\n",
    "    \n",
    "    return critic_loss.item(), actor_loss.item()\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "def play_game(model1, model2, env, num_episodes):\n",
    "    total_score = 0\n",
    "    for episode in range(num_episodes):\n",
    "        # print(f\"\\nEpisode {episode + 1}/{num_episodes}\")\n",
    "        state = env.get_state().to(device)\n",
    "        done = False\n",
    "        moves = 0\n",
    "        \n",
    "        while not done:\n",
    "            moves += 1\n",
    "            # Model 1's turn\n",
    "            action1 = model1(state)\n",
    "            reward, done = env.step(action1)\n",
    "            total_score += reward\n",
    "            # print(f\"Move {moves}: Model 1 reward: {reward}\")\n",
    "            \n",
    "            if done:\n",
    "                print(f\"Game ended after {moves} moves\")\n",
    "                break\n",
    "                \n",
    "            # Model 2's turn    \n",
    "            state = env.get_state().to(device)\n",
    "            action2 = model2(state)\n",
    "            reward, done = env.step(action2)\n",
    "            total_score -= reward\n",
    "            # print(f\"Move {moves + 1}: Model 2 reward: {reward}\")\n",
    "            \n",
    "            if not done:\n",
    "                state = env.get_state().to(device)\n",
    "                \n",
    "        # print(f\"Episode {episode + 1} complete - Total score: {total_score}\")\n",
    "    \n",
    "    return total_score\n",
    "\n",
    "def save_champion(actor, critic, folder_path):\n",
    "    # Save directly without the 'model_state_dict' wrapper\n",
    "    torch.save(actor.state_dict(), f\"{folder_path}/champion_actor.pth\")\n",
    "    torch.save(critic.state_dict(), f\"{folder_path}/champion_critic.pth\")\n",
    "\n",
    "def test_champion(folder_path):\n",
    "    # Load the champion model\n",
    "    actor = ActorCNN(output_size=4672).to(device)\n",
    "    actor.load_state_dict(torch.load(f'{folder_path}/champion_actor.pth'))\n",
    "    actor.eval()\n",
    "    \n",
    "    print(f\"\\nTesting champion from {folder_path}\")\n",
    "    game = play_chess_game(actor, device)  # Pass device as well\n",
    "    \n",
    "    pgn_path = f'{folder_path}/champion_game.pgn'\n",
    "    with open(pgn_path, 'w') as f:\n",
    "        f.write(str(game))\n",
    "\n",
    "def get_model_move(model, board):\n",
    "    with torch.no_grad():\n",
    "        state_3d = board_to_3d_tensor(board).unsqueeze(0).to(device)\n",
    "        action_probs = model(state_3d)  # shape (1, 4672)\n",
    "        move_idx = torch.argmax(action_probs).item()\n",
    "        legal_moves = list(board.legal_moves)\n",
    "        return legal_moves[move_idx % len(legal_moves)]\n",
    "\n",
    "\n",
    "def get_model_move(model, state, board):\n",
    "    with torch.no_grad():\n",
    "        action_probs = model(state)\n",
    "        move_idx = torch.argmax(action_probs).item()\n",
    "        legal_moves = list(board.legal_moves)\n",
    "        return legal_moves[move_idx % len(legal_moves)]\n",
    "\n",
    "def play_chess_game(model, device):\n",
    "    board = chess.Board()\n",
    "    game = chess.pgn.Game()\n",
    "    node = game\n",
    "    \n",
    "    while not board.is_game_over():\n",
    "        # Convert board to state tensor\n",
    "        state = board_to_3d_tensor(board).unsqueeze(0).to(device)\n",
    "        # Pass the state tensor and the board to get_model_move\n",
    "        move = get_model_move(model, state, board)\n",
    "        board.push(move)\n",
    "        node = node.add_variation(move)\n",
    "    \n",
    "    return game\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Generation 1_3\n",
      "Using device: cpu\n",
      "\n",
      "Episode 1/100\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Train each model in population\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (actor, critic) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(population\u001b[38;5;241m.\u001b[39mpopulation):\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mtrain_chess_ai\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactor_net\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcritic_net\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Run tournament to find best model\u001b[39;00m\n\u001b[1;32m     57\u001b[0m rankings \u001b[38;5;241m=\u001b[39m run_tournament(population\u001b[38;5;241m.\u001b[39mpopulation)\n",
      "Cell \u001b[0;32mIn[8], line 80\u001b[0m, in \u001b[0;36mtrain_chess_ai\u001b[0;34m(num_episodes, batch_size, gamma, actor_net, critic_net, save_path)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(board)\n\u001b[1;32m     79\u001b[0m state \u001b[38;5;241m=\u001b[39m board_to_3d_tensor(board)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m move \u001b[38;5;241m=\u001b[39m \u001b[43mmcts_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimulations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m board\u001b[38;5;241m.\u001b[39mpush(move)\n\u001b[1;32m     82\u001b[0m next_state \u001b[38;5;241m=\u001b[39m board_to_3d_tensor(board)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 100\u001b[0m, in \u001b[0;36mmcts_search\u001b[0;34m(board, actor_net, critic_net, num_simulations)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mis_game_over():\n\u001b[1;32m     98\u001b[0m     node\u001b[38;5;241m.\u001b[39mexpand(actor_net)\n\u001b[0;32m--> 100\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43mcritic_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m search_path:\n\u001b[1;32m    103\u001b[0m     node\u001b[38;5;241m.\u001b[39mvalue_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 68\u001b[0m, in \u001b[0;36mCriticCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mx shape: (batch_size, 12, 8, 8)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))\n\u001b[0;32m---> 68\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))\n\u001b[1;32m     71\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main generational loop\n",
    "major_gen = 1\n",
    "minor_gen = 3\n",
    "max_minor_gen = 1\n",
    "max_major_gen = 4\n",
    "\n",
    "def create_generation_folder(gen_major, gen_minor):\n",
    "    folder_name = f\"chessCNN_ai_generation_{gen_major}_{gen_minor}\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    return folder_name\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_previous_champion(prev_major_gen, prev_minor_gen):\n",
    "    prev_folder = f\"chessCNN_ai_generation_{prev_major_gen}_{prev_minor_gen}\"\n",
    "    actor_path = os.path.join(prev_folder, 'champion_actor.pth')\n",
    "    critic_path = os.path.join(prev_folder, 'champion_critic.pth')\n",
    "    \n",
    "    actor = ActorCNN(output_size=4672).to(device)\n",
    "    critic = CriticCNN().to(device)\n",
    "    \n",
    "    actor.load_state_dict(torch.load(actor_path))\n",
    "    critic.load_state_dict(torch.load(critic_path))\n",
    "    \n",
    "    return actor, critic\n",
    "\n",
    "while major_gen <= max_major_gen:\n",
    "    current_folder = create_generation_folder(major_gen, minor_gen)\n",
    "    print(f\"\\nStarting Generation {major_gen}_{minor_gen}\")\n",
    "    \n",
    "    # Load previous champion or create fresh models\n",
    "    if major_gen == 2 and minor_gen == 1:\n",
    "        initial_actor, initial_critic = load_previous_champion(1, 1)\n",
    "    else:\n",
    "        initial_actor = ActorCNN(output_size=4672).to(device)\n",
    "        initial_critic = CriticCNN().to(device)\n",
    "    \n",
    "    # Create population with mutations\n",
    "    population = ModelPopulation(population_size=8)\n",
    "    population.population[0] = (initial_actor, initial_critic)\n",
    "    \n",
    "    for i in range(1, len(population.population)):\n",
    "        population.population[i] = (\n",
    "            mutate_network(initial_actor, mutation_rate=0.01).to(device),\n",
    "            mutate_network(initial_critic, mutation_rate=0.01).to(device)\n",
    "        )\n",
    "    \n",
    "    # Train each model in population\n",
    "    for i, (actor, critic) in enumerate(population.population):\n",
    "        train_chess_ai(\n",
    "            actor_net=actor,\n",
    "            critic_net=critic,\n",
    "            save_path=os.path.join(current_folder, f'model_{i}')\n",
    "        )\n",
    "    \n",
    "    # Run tournament to find best model\n",
    "    rankings = run_tournament(population.population)\n",
    "    best_model_idx = rankings[0][0]\n",
    "    winner_actor, winner_critic = population.population[best_model_idx]\n",
    "    \n",
    "    # Save champion\n",
    "    save_champion(winner_actor, winner_critic, current_folder)\n",
    "    \n",
    "    # Test the champion\n",
    "    test_champion(current_folder)\n",
    "    \n",
    "    # Update generation counters\n",
    "    minor_gen += 1\n",
    "    if minor_gen > max_minor_gen:\n",
    "        minor_gen = 1\n",
    "        major_gen += 1\n",
    "    \n",
    "    print(f\"Completed Generation {major_gen}_{minor_gen-1}\")\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
